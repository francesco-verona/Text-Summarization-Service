{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9f048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scarica i dati dall' HUB e li salva in cache come oggetto DatasetDict\n",
    "from datasets import load_dataset, DatasetDict\n",
    "cnn_dataset = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
    "small_cnn_dataset = DatasetDict({\n",
    "    \"train\": cnn_dataset[\"train\"].select(range(5000)),\n",
    "    \"validation\": cnn_dataset[\"validation\"].select(range(1000)),\n",
    "    \"test\": cnn_dataset[\"test\"].select(range(1000)),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba95a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             article  \\\n",
      "0  LONDON, England (Reuters) -- Harry Potter star...   \n",
      "1  Editor's note: In our Behind the Scenes series...   \n",
      "2  MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n",
      "3  WASHINGTON (CNN) -- Doctors removed five small...   \n",
      "4  (CNN)  -- The National Football League has ind...   \n",
      "\n",
      "                                          highlights  \\\n",
      "0  Harry Potter star Daniel Radcliffe gets £20M f...   \n",
      "1  Mentally ill inmates in Miami are housed on th...   \n",
      "2  NEW: \"I thought I was going to die,\" driver sa...   \n",
      "3  Five small polyps found during procedure; \"non...   \n",
      "4  NEW: NFL chief, Atlanta Falcons owner critical...   \n",
      "\n",
      "                                         id  \n",
      "0  42c027e4ff9730fbb3de84c1af0d2c506e41c3e4  \n",
      "1  ee8871b15c50d0db17b0179a6d2beab35065f1e9  \n",
      "2  06352019a19ae31e527f37f7571c6dd7f0c5da37  \n",
      "3  24521a2abb2e1f5e34e6824e0f9e56904a2b0e88  \n",
      "4  7fe70cc8b12fab2d0a258fababf7d9c6b5e1262a  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Prendo lo split train (è un Dataset)\n",
    "train_split = small_cnn_dataset[\"train\"]\n",
    "\n",
    "# Conversione a pandas DataFrame\n",
    "df = train_split.to_pandas()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909ff15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch, tokenizer, prompt: str = \"\",  max_input_len=512, max_target_len=128):\n",
    "    # Per ogni riga del batch si organizza l'input in base alla presenza o meno del prompt (essenziale per LLM GPT)\n",
    "    inputs = [f\"{prompt}{doc}\" for doc in batch[\"article\"]]\n",
    "\n",
    "    # Tokenizza l'input\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_len,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Tokenizza il target (riassunto)\n",
    "    labels = tokenizer(\n",
    "        batch[\"highlights\"],\n",
    "        max_length=max_target_len,\n",
    "        truncation=True\n",
    "    )\n",
    "    # se nell' etichetta abbiamo padding questo valore viene ignorato nel calcolo della LOSS\n",
    "    labels_ids = labels[\"input_ids\"]\n",
    "    labels_ids = [\n",
    "        [(id if id != tokenizer.pad_token_id else -100) for id in seq]\n",
    "        for seq in labels_ids\n",
    "    ]\n",
    "\n",
    "    # Aggiunge labels al dizionario\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998a47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate \n",
    "\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d983a931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb1ace45c974f51bb4d18bb706d8d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# configurazione del modello\n",
    "model_checkpoint = \"google-t5/t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenized_dataset = small_cnn_dataset.map(\n",
    "    lambda batch: preprocess(\n",
    "        batch,\n",
    "        tokenizer=tokenizer\n",
    "        # ,prompt=\"Riassumi: \", \n",
    "    ),\n",
    "    batched=True\n",
    ")\n",
    "#configurazione degli iperparametri\n",
    "batch_size = 2\n",
    "num_train_epochs = 8\n",
    "logging_steps = len(tokenized_dataset[\"train\"])\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = f\"{model_name}\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate = 5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub = False,\n",
    "    report_to=\"none\" \n",
    ")\n",
    "\n",
    "# data collator\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer,    model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46247cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9e661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7648/915633162.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/root/NLP_sumarization/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1962' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1962/20000 1:47:36 < 16:30:21, 0.30 it/s, Epoch 0.78/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cc3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_sumarization (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
